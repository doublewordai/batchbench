#!/usr/bin/env bash
set -euo pipefail

IMAGE_NAME=${IMAGE_NAME:-batchbench-cu126:latest}

# Shared directory for generated request payloads and logs
OUTPUT_DIR=${OUTPUT_DIR:-$(pwd)/outputs}
mkdir -p "$OUTPUT_DIR"
rm -f "$OUTPUT_DIR/requests.jsonl"

export BATCHBENCH_MODE=online

# vLLM server configuration
export ONLINE_MODEL=${ONLINE_MODEL:-facebook/opt-125m}
export ONLINE_TENSOR_PARALLEL_SIZE=${ONLINE_TENSOR_PARALLEL_SIZE:-1}
export ONLINE_PIPELINE_PARALLEL_SIZE=${ONLINE_PIPELINE_PARALLEL_SIZE:-1}
export ONLINE_MAX_NUM_BATCHED_TOKENS=${ONLINE_MAX_NUM_BATCHED_TOKENS:-1024}

# Dataset generation options
export ONLINE_DATASET_PATH=${ONLINE_DATASET_PATH:-/outputs/requests.jsonl}
export ONLINE_GENERATE_COUNT=${ONLINE_GENERATE_COUNT:-32}
export ONLINE_GENERATE_PREFIX_OVERLAP=${ONLINE_GENERATE_PREFIX_OVERLAP:-0.0}
export ONLINE_GENERATE_APPROX_INPUT_TOKENS=${ONLINE_GENERATE_APPROX_INPUT_TOKENS:-256}

# Online benchmark client parameters
export ONLINE_CLIENT_MODEL=${ONLINE_CLIENT_MODEL:-$ONLINE_MODEL}
export ONLINE_USERS=${ONLINE_USERS:-4}
export ONLINE_REQUESTS_PER_USER=${ONLINE_REQUESTS_PER_USER:-1}
export ONLINE_REQUEST_TIMEOUT_SECS=${ONLINE_REQUEST_TIMEOUT_SECS:-120}

exec docker run --rm \
  --gpus all \
  -p 8000:8000 \
  -e BATCHBENCH_MODE \
  -e ONLINE_MODEL \
  -e ONLINE_TENSOR_PARALLEL_SIZE \
  -e ONLINE_PIPELINE_PARALLEL_SIZE \
  -e ONLINE_MAX_NUM_BATCHED_TOKENS \
  -e ONLINE_DATASET_PATH \
  -e ONLINE_GENERATE_COUNT \
  -e ONLINE_GENERATE_PREFIX_OVERLAP \
  -e ONLINE_GENERATE_APPROX_INPUT_TOKENS \
  -e ONLINE_CLIENT_MODEL \
  -e ONLINE_USERS \
  -e ONLINE_REQUESTS_PER_USER \
  -e ONLINE_REQUEST_TIMEOUT_SECS \
  -v "$OUTPUT_DIR":/outputs \
  "$IMAGE_NAME"
